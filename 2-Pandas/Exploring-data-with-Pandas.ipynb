{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring data with Pandas - Part 1\n",
    "Pandas is an exceptional tool for working with tabular data. Here we review the basics of what a dataframe is, how we get data into a dataframe, and how we can explore those data when in a dataframe format.\n",
    "### Topics\n",
    "1. [Basic form of a dataframe](#1.-The-DataFrame)\n",
    "* [Loading data into a dataframe](#2.-Loading-data-into-a-dataframe)\n",
    "* [Viewing and inspecting data properties](#3.-Viewing-and-inspecting-data-properties)\n",
    "* [Selecting columns](#4.-Selecting-columns)\n",
    "* [Descriptive statistics](#5.-Descriptive-Statistics)\n",
    "* [Some basic plots](#6.-Some-Basic-Plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the pandas package, conventially imported as \"pd\"\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The DataFrame\n",
    "We'll begin by exploring the key elements of the DataFrame object. Some notions are self evident, i.e., data are stored in rows and columns, much like a spreadsheet. Others are more nuanced: implicit and explicit indices, tables vs. views, and some others.\n",
    "\n",
    "Let's begin examining the components of DataFrames by examining two ways they can be created.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame as a list of lists\n",
    "First, a DataFrame can be considered as a list of lists. Below we see an example where we have 4 sub-lists, each containing 3 items (e.g. the first list [`'Joe'`,`22`,& `True`]). Each of these 4 sub-lists comprises a _row_ in the resulting DataFrame, and each item in a given list becomes a _column_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a simple data frame as a list of lists\n",
    "df = pd.DataFrame([['Joe',22,True],\n",
    "                   ['Bob',25,False],\n",
    "                   ['Sue',28,False],\n",
    "                   ['Ken',24,True]],\n",
    "                  index = [10,20,40,30],\n",
    "                  columns = ['Name','Age','IsStudent']\n",
    "                 )\n",
    "#Reveal the type of object created\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the resulting data frame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few key points here: \n",
    "* First is that each of the sub-lists has the same number of elements (3) and the same data types as the other sub-lists. Otherwise we'd end up with missing data or \"coerced\" data types.\n",
    "* Second is that we also explicitly specify and **index** for the rows (`index = [1,2,3,4]`). The index allows us to identify a specific row.\n",
    "* Likewise, we explicitly set column names with `columns = ['Name','Age','IsStudent']`, and yes, these allow us to indentify specific columns in our DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data frame as a collection of dictionaries\n",
    "Another way to build (and think of) a DataFrame as a set of dictionaries where each dictionary is a column of data, with the dictionary's key being the column name and it's value being a list of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a data frame as dictionaries of lists\n",
    "df = pd.DataFrame({\"Name\":['Joe','Bob','Sue','Ken'],\n",
    "                   \"Age\":[22,25,28,24],\n",
    "                   \"IsStudent\":[True,False,False,True]},\n",
    "                  index = [10,20,40,30]\n",
    "                 )\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meh, so what...\n",
    "What does this reveal? List of lists vs set of dictionaries? Well, it explains how you can extract elements from the DataFrame. **Thinking of a DataFrame as a list of lists**, getting the value of the 2nd column, 3rd row is equivalent of getting data from the 2nd item in the 3rd list.  \n",
    "\n",
    "We can get that value using the DataFrame's `iloc` function (short for intrinsic location), passing the row and column of the location we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the 2nd item from the 3rd row; recalling Python is zero-based\n",
    "df.iloc[2,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we hop over to **thinking a DataFrame as a set of dictionaries**, we can target a specific value by specifying the index of the value (row) from the dictionary column) we want. The row however, is referred to by the index *we* assigned, not it's implicit index generated by the order in which it was entered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the value in the 'Name' column corresponding to the row with an index of '20'\n",
    "df['Name'][20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also to quick math on our data. If we wanted to calculate the age of our students in days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age_days'] = df['Age'] * 365\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We'll return to how we extract data from a DataFrame, but for now just soak in the fact that values in a DataFrame can be referenced by their implicit location (i.e. their row, column coordinates) and by their explicit column name and row index._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Loading data into a dataframe\n",
    "More than likely we'll be reading in data vs entering it manually, so let's review how files are read into a Pandas Dataframe. Pandas can read many other formats: Excel files, HTML tables, JSON, etc. But let's concentrate on the simplest one - the csv file - and discuss the key parameters involved. \n",
    "\n",
    "In the Data folder within our workspace is a file named `surveys.csv` which holds the data we'll use. If you're curious, this dataset is part of the Portal Teaching data, a subset of the data from Ernst et al [Long-term monitoring and experimental manipulation of a Chihuahuan Desert ecosystem near Portal, Arizona, USA](http://www.esapubs.org/archive/ecol/E090/118/default.htm).\n",
    "\n",
    "The dataset is stored as a `.csv` file: each row holds information for a single animal, and the columns represent:\n",
    "\n",
    "| Column | Description |\n",
    "| :--- | :--- |\n",
    "|record_id |\tUnique id for the observation |\n",
    "|month| \tmonth of observation |\n",
    "|day |\tday of observation |\n",
    "|year |\tyear of observation |\n",
    "|plot_id |\tID of a particular plot |\n",
    "|species_id |\t2-letter code |\n",
    "|sex |\tsex of animal (“M”, “F”) |\n",
    "|hindfoot_length |\tlength of the hindfoot in mm |\n",
    "|weight |\tweight of the animal in grams |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we read in this file, saving the contents to the variabel `surveys_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the surveys.csv file\n",
    "surveys_df = pd.read_csv('../data/surveys.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the dataframe just by typing the variable name\n",
    "surveys_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas can read data stored in many different formats. CSV is one of the more common ones, but it can also handle txt, JSON, HTML, MS Excel, HDF5, Stata, SAS, and certain SQL formats. Each has it's own `read_()` function and is fully documented. \n",
    "\n",
    "* Run the following to get documentation for the `read_csv()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display help on the read_csv() function\n",
    "pd.read_csv?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full documentation can be found simply via a web search on \"[pandas read_csv](https://www.google.com/search?q=read_csv)\".\n",
    "\n",
    ">Note all the options available to read in a simple CSV file. Think back to some of the other files we've read into Python using its file object. How might the following options have helped? \n",
    "* `delimeter` or `sep` to read in tab delimeted files...\n",
    "* `comment` to skip metadata rows...\n",
    "* `skiprows` to skip metadata rows...\n",
    "\n",
    "I've never used many of these modifiers, but it's important to know they exist and how they are implemented. I often use a bit of trial and error when first applying them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the `dtype` modifier \n",
    "One important modifier that is easy to overlook is the `dtype` one. This modifier allows us to override the default data type that Pandas assignes to a column when it's imported into a dataframe. \n",
    "\n",
    "Note that in our `surveys_df` dataframe, we have two numeric columns that contain _nominal_ data: `record_id` and `plot_id`. It's possible these numeric labels may have leading zeros. (Think ZIP codes or HUC codes...). As such, we'd want to be sure to import these values as _strings_, not _integers_ as Pandas would do by default. \n",
    "\n",
    "We do this using the `dtype` modifier, passing a dictionary of column name:format for each variable we want to ensure is imported under our control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the surveys.csv file\n",
    "surveys_df = pd.read_csv(\n",
    "    '../data/surveys.csv',\n",
    "    dtype={'record_id':'str','plot_id':'str'})\n",
    "surveys_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Viewing and inspecting data properties\n",
    "We've already seen that typing in the dataframe's variable name will display a nicely formatted snapshot of the data, truncated if the dataframe is too big. \n",
    "\n",
    "### Viewing data with `head()`, `tail()`, and `sample()`\n",
    "Some handy commands to show snippets of our dataframe are `head()`, `tail()`, and `sample()`. We can pass in a number as the argument to each of these to display a set number of records from our dataset. (`head()` and `tail()` default to 5.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the head() command to view the first 5 rows of the dataframe\n",
    "surveys_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "► What do you think the sample function does? What about sample? How might you find out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try out the tail command\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try out the sample command (you need to specifiy number of records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inpsecting our dataframe's properties\n",
    "Here, we'll create a second dataframe by reading in a dataset stored online. And then we'll apply the following commands to explore properties of this dataset:\n",
    "* Revealing size attributes with `len()`, `shape`, and `size`\n",
    "* Revealing columns included with `columns`\n",
    "* Revealing the index with `index`\n",
    "* Revealing the dataframe's data types with `dtypes` and `info()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in HUC12 land cover data from EPA's EnviroAtlas dataset\n",
    "data_url = 'https://github.com/ENV859/EnviroAtlasData/blob/main/LandCover.csv?raw=true'\n",
    "land_df = pd.read_csv(data_url)\n",
    "land_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass our dataframe into the len() function\n",
    "len(land_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reveal the shape of the dataframe\n",
    "land_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reveal the size of the dataframe\n",
    "land_df.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "► Can you deduce what properties the `len()`, `shape`, and `size` reveal? \n",
    "* How many rows does the dataframe have?\n",
    "* How many columns?\n",
    "* How many total values are in this table? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show a list of columns in the dataframe\n",
    "land_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the dataframe's index\n",
    "land_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the data types of each column \n",
    "land_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show more information on the dataframe and on each column\n",
    "land_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Check your understanding:\n",
    "1. Using the commands describe above, answer the following questions regarding the **surveys_df** dataframe.\n",
    " 1. How many records (rows) are in the dataset?\n",
    " * How many columns are in the dataset?\n",
    " * What are the column names?\n",
    " * What data type do the columns use?\n",
    " * How many total values are stored in this dataframe?\n",
    " * What are the indices used in this dataframe?\n",
    " * How many non-null values are found in the 'WEIGHT column  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many records (rows) are in the dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many columns are in the dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the column names?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What data type do the columns use?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many total values are stored in this dataframe?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the indices used in this dataframe?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many non-null values are found in the 'WEIGHT column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Re-read EnviroAtlas data into the **land_df** dataframe, but this time ensure the `HUC_12` column is read as a **string**, not as an **integer (int64)**.\n",
    " * Ensure that the data type of the column was read in as an \"object\" (i.e. a string) not \"int64\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-read EnviroAtlas data into the land_df dataframe, ensuring HUC_12 is a string\n",
    "data_url = 'https://github.com/ENV859/EnviroAtlasData/blob/main/LandCover.csv?raw=true'\n",
    "land_df = pd.read_csv(data_url) #<-- Modify this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the data type of the column was read in as an \"object\" (i.e. a string) not \"int64\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Selecting columns\n",
    "We can select specific columns based on the column names. The basic syntax is dataframe[column name], where value can be a single column name, or a _list_ of column names. Let’s start by selecting two columns from our `surveys_df` dataframe, `species_id` and `hindfoot_length`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset just the two columns from the full dataframe\n",
    "selection = surveys_df[['species_id','hindfoot_length']]\n",
    "selection.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reveal what is returned - it's a dataframe\n",
    "type(selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reveal the shape of the new dataframe\n",
    "selection.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: if we select just one column, the obejct returns is a **series**, not a dataframe. \n",
    ">A **series** is simply one column of data. However, it has a different set of properties and methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_col = surveys_df['hindfoot_length']\n",
    "one_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(one_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**NOTE**: You can also retreive a column using a different syntax:\n",
    "```python\n",
    "surveys_df.hindfoot_length\n",
    "```\n",
    "This syntax works only if the column name is a valid name for a Python variable (e.g. the column name should not contain whitespace). The syntax data[\"column\"] works for all kinds of column names, so we recommend using this approach. Also, things may get ugly if you have a column name that conflicts with a property of your dataframe. For example, what if your column name was \"`shape`\"??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Descriptive Statistics\n",
    "Pandas DataFrames and Series contain useful methods for getting summary statistics. Available methods include `mean()`, `median()`, `min()`, `max()`, and `std()` (the standard deviation).\n",
    "\n",
    "We could, for example, check the mean hindfoot length in our input data. We check the mean for a single column (Series):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the mean hindfoot length\n",
    "surveys_df['hindfoot_length'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try computing some other summary stats for either of our two numeric columns (`hindfoot_lenght` or `weight`) using the statements mentioned above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the count of weight records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the standard deviation of weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the median weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What were the first and last years of the survey\n",
    "first_year =\n",
    "last_year =\n",
    "print(f\"The study spanned fro {first_year} to {last_year}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few more complex ones are **quantiles** and **correlations**:\n",
    "\n",
    "→ First, percentiles using the `quantiles()` function, where we pass in the percent to compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute a the 25th percentile of weight\n",
    "surveys_df['weight'].quantile(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the above to compute the 50th percentile; does it equal the median?\n",
    "surveys_df['weight'].median() == surveys_df['weight'].quantile(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→ Now to compute **variable pairwise coorelations**. Here we'll use the HUC12 land cover data and display pairwise correlation values among its variables (using the default Pearson method as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pairwise correlation table among variables in the land_df dataframe\n",
    "land_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TIP**: Using visualization tricks that we'll touch on later, we can style our table, making it much more informative..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the correlation table to a variable\n",
    "coorelation_table = land_df.corr()\n",
    "#Show the table, with styling\n",
    "coorelation_table.style.background_gradient(cmap = 'YlGnBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas also as a `describe()` function that quickly generates descriptive stats for all numeric variables in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate summary stats on all numeric columns in the data\n",
    "land_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical data\n",
    "Descriptive statistics are useful for numeric data. For categorical data, we have other means for summarizing data:\n",
    "* `nunique()`: lists the number of unique values in each column of a dataframe (or in a series)\n",
    "* `unique()` : lists the unique values occuring in the supplied series\n",
    "* `value_counts()` : lists the number of records associated with each unique value\n",
    "\n",
    "_→In exploring these, consider the type of object they return and what you can do with that object..._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reveal how many unique values in each field in the surveys_df dataframe\n",
    "surveys_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reveal how many unique values occur just in the 'species_id' series\n",
    "surveys_df['species_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List the unique values in the species_id field\n",
    "surveys_df['species_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List how many records are associated with each unique month record\n",
    "surveys_df['month'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <font color='red'>*Challenge* - Counts and Lists from Data </font>\n",
    "\n",
    "\n",
    "1. Create a list of unique **plot ID**’s found in the surveys data (much like we did above with the species id values). Call it `plot_names`. How many unique plots are there in the data? How many unique species are in the data?\n",
    "\n",
    "1. What is the difference between `len(plot_names)` and `surveys_df['plot_id'].nunique()`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Some Basic Plots\n",
    "We'll revist visualizations, but as a teaser here are some quick and easy plots of our data. \n",
    "\n",
    "* First a histogram of all the `PFOR` (percent forest) values from the EnviroAtlas data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram of values in the PFOR column, in 10 bins\n",
    "land_df['PFOR'].hist(bins=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* And now, we'll generate histograms for each month of the `weight` values in the `surveys_df` dataframe. We'll also increase the size of the figure to 20 x 20 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot distributions of weights by month\n",
    "surveys_df.hist(column='weight',by='month',figsize=(20,20));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And finally box plots by month\n",
    "surveys_df.boxplot(column='weight',by='species_id',figsize=(20,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "While this is a very quick introduction to the Pandas dataframe and series objects, I'm hopeful that you now have an appreciation for the utility of structuring data into a dataframe has -- and now have some command on how to generate and explore dataframes using the Pandas package. \n",
    "\n",
    "Actions you should now be capable of include:\n",
    "* Creating a dataframe from a lists of lists (of equal sizes)\n",
    "* Creating a dataframe from a set of dictionairies (of equal lengths)\n",
    "* Reading data from a CSV file into a Pandas dataframe\n",
    "* Viewing and inspecting properties of the data in your dataframe\n",
    "* Selecting specific columns in your dataframe to a new object\n",
    "* Computing descriptive statistics from your data\n",
    "* Generating a few basic plots\n",
    "\n",
    "### Next up: \n",
    "Next we will explore how we can effectively process data stored in dataframes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
